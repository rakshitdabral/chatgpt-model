{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4ce8d2f-be57-48ac-bc51-f21cb69a4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0b54-a5e4-4c3e-851c-76d3dc90eb7d",
   "metadata": {},
   "source": [
    "### Obtaining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcf7c71-9a44-4e5f-8dac-9c35fa2abfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = tf.keras.utils.get_file(\n",
    "    fname='fra-eng.zip',\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22e579a-9778-4e51-9b3f-74bf5f2b68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path(text_file).parent / 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "516c7bd1-3875-4cb0-a11a-d36d1aa1e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(line):\n",
    "    # Normalize unicode characters, strip leading/trailing whitespace, convert to lowercase\n",
    "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
    "    # Handle special characters and add start and end tokens for the target language (French)\n",
    "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\"\\1\", line)\n",
    "    eng, fre = line.split(\"\\t\")\n",
    "    fre = '[start] ' + fre + ' [end]'\n",
    "    return eng, fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9254617c-58f3-4ee5-9048-e07cb48fc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as fp:\n",
    "    text_pairs = [normalize(line) for line in fp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b214b8e-1f9f-4ac2-a7a5-aaa81b901f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique tokens for English and French\n",
    "eng_tokens, fre_tokens = set(), set()\n",
    "# Initialize variables to store maximum sequence lengths\n",
    "eng_maxlen, fre_maxlen = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb33934e-fc74-46b1-b2e2-7cdd2148651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, fre in text_pairs:\n",
    "    eng_token, fre_token = eng.split(), fre.split()\n",
    "    eng_maxlen = max(eng_maxlen, len(eng_token))\n",
    "    fre_maxlen = max(fre_maxlen, len(fre_token))\n",
    "    eng_tokens.update(eng_token)\n",
    "    fre_tokens.update(fre_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9b0a45-84db-4f3c-a686-ad30fa5c8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in English: 25380\n",
      "Total tokens in French: 44866\n",
      "Maximum length of English sequence: 47\n",
      "Maximum length of French sequence: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens in English: {len(eng_tokens)}\")\n",
    "print(f\"Total tokens in French: {len(fre_tokens)}\")\n",
    "print(f\"Maximum length of English sequence: {eng_maxlen}\")\n",
    "print(f\"Maximum length of French sequence: {fre_maxlen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978d6d-9a57-4347-8a31-ab30b91de8cd",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27ab2d3-794f-411d-b063-33a9ff454219",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_pairs.pickle\", 'wb') as fp:\n",
    "    pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f11224-f48e-4835-904d-519a018676e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed text pairs\n",
    "with open(\"text_pairs.pickle\", 'rb') as fp:\n",
    "    text_pairs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f946f7a-816e-4d68-b532-7e477a8e0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "random.shuffle(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe1787b1-a9a2-4a16-8d26-82dfdf1b26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "n_val = int(0.15 * len(text_pairs))\n",
    "n_train = len(text_pairs) - 2 * n_val\n",
    "train_pair = text_pairs[:n_train]\n",
    "test_pair = text_pairs[n_train: n_train + n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c993ee-7a63-489c-ad31-c387eb4645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary sizes and sequence length\n",
    "vocab_en = 10000\n",
    "vocab_fr = 20000\n",
    "seq_length = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55861ff-beed-4f6d-bb48-9d5b46270d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextVectorization layers\n",
    "eng_vect = TextVectorization(\n",
    "    max_tokens=vocab_en,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length\n",
    ")\n",
    "\n",
    "fre_vect = TextVectorization(\n",
    "    max_tokens=vocab_fr,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length + 1  # +1 for start token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2021ba26-e63c-4931-9ec2-a5a44d97313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt TextVectorization layers to training data\n",
    "train_eng = [pair[0] for pair in train_pair]\n",
    "train_fre = [pair[1] for pair in train_pair]\n",
    "\n",
    "eng_vect.adapt(train_eng)\n",
    "fre_vect.adapt(train_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68f7a14a-7e1c-4aad-9b2b-e9602fb1fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the vectorization layers and training/test data\n",
    "with open('vectorize.pickle', 'wb') as fp:\n",
    "    data = {'train': train_pair,\n",
    "            'test': test_pair,\n",
    "            'eng_vect': eng_vect.get_config(),\n",
    "            'fre_vect': fre_vect.get_config(),\n",
    "            'eng_weights': eng_vect.get_weights(),\n",
    "            'fre_weights': fre_vect.get_weights()\n",
    "            }\n",
    "    pickle.dump(data, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052d86ff-b167-48c1-b4de-8d6db30240ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load serialized data\n",
    "with open(\"vectorize.pickle\", 'rb') as fp:\n",
    "    data = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e925250e-ffe3-4916-a5a9-e39c5fc1e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve train and test pairs\n",
    "train_pair = data['train']\n",
    "test_pair = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05aa9171-3bc1-47a1-b716-9c78ece29132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct TextVectorization layers\n",
    "eng_vect = TextVectorization.from_config(data['eng_vect'])\n",
    "eng_vect.set_weights(data['eng_weights'])\n",
    "fre_vect = TextVectorization.from_config(data['fre_vect'])\n",
    "fre_vect.set_weights(data['fre_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b73a4ff0-5d53-4462-9b6b-184a47540f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to format dataset\n",
    "def format_dataset(eng, fre):\n",
    "    eng = eng_vect(eng)\n",
    "    fre = fre_vect(fre)\n",
    "    source = {'encode_inp': eng,\n",
    "              'decode_inp': fre[:, :-1]\n",
    "              }\n",
    "    target = fre[:, 1:]\n",
    "    return (source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eb168b6-9a73-449c-a885-74fee8163a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create dataset\n",
    "def make_dataset(pairs, batchsize=64):\n",
    "    eng_text, fre_text = zip(*pairs)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_text), list(fre_text)))\n",
    "    return dataset.shuffle(2048).batch(batchsize).map(format_dataset).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f202c5b-ae91-4dc3-b8e2-37c08c65b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for training and testing\n",
    "train_ds = make_dataset(train_pair)\n",
    "test_ds = make_dataset(test_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78d68-cbef-454c-9606-0678f137d722",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28120abd-3f4d-4b8d-a200-dbfdecbf4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate positional encoding matrix\n",
    "def pos_enc_matrix(L, d, n=10000):\n",
    "    assert d % 2 == 0\n",
    "    d2 = d // 2\n",
    "\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)\n",
    "    i = np.arange(d2).reshape(1, -1)\n",
    "\n",
    "    denom = np.power(n, -i / d2)\n",
    "    args = k * denom\n",
    "\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855d0ffc-5761-4b75-abc4-33d3b5f82ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras layer for positional embedding\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, seq_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        matrix = pos_enc_matrix(seq_length, embed_dim)\n",
    "\n",
    "        self.positional_embedding = tf.constant(matrix, dtype='float32')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + self.positional_embedding\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'seq_length': self.seq_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embed_dim': self.embed_dim\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95d2f661-fa2d-45cf-b008-704c996e1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " ...\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]], shape=(64, 25), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Usage and Validation\n",
    "vocab_en = 10000\n",
    "seq_length = 25\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    embed_en = PositionalEmbedding(seq_length, vocab_en, embed_dim=512)\n",
    "    en_emb = embed_en(inputs['encode_inp'])\n",
    "    print(en_emb._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813b72c-28cc-48da-878c-2958c3648a9b",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d33ec-bf26-461e-8d79-5acffd5a0fc6",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd3455a4-f73b-4bdd-a10e-38a013f16e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_shape, prefix='att', mask=False, **kwargs):\n",
    "    # Define inputs\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in1\")\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_att1\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm1')\n",
    "    add = tf.keras.layers.Add(name=f'{prefix}_add1')\n",
    "\n",
    "    # Apply attention mechanism\n",
    "    attout = attention(query=inputs, value=inputs, key=inputs, use_causal_mask=mask)\n",
    "\n",
    "    # Apply normalization and residual connection\n",
    "    output = norm(add([inputs, attout]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f\"{prefix}_att\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781af8f9-3361-41af-aafa-e20e0a701838",
   "metadata": {},
   "source": [
    "### Cross-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d1f1ce-e59d-4452-a008-49f32af6bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(input_shape, context_shape, prefix='att', **kwargs):\n",
    "    # Define inputs\n",
    "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32', name=f\"{prefix}_ctx2\")\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in2')\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f'{prefix}_att2', **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm2')\n",
    "    add = tf.keras.layers.Add(name=f'{prefix}_add2')\n",
    "\n",
    "    # Apply attention mechanism\n",
    "    attout = attention(query=inputs, key=context, value=context)\n",
    "\n",
    "    # Apply normalization and residual connection\n",
    "    output = norm(add([attout, inputs]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[context, inputs], outputs=output, name=f'{prefix}_crs_at')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a3776-b54d-4c2e-81a1-b36dab731ccf",
   "metadata": {},
   "source": [
    "### Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7382b5a8-6a69-4f87-9d5d-0f5cfa75d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(input_shape, model_dim, ff_dim, dropout=.1, prefix='ff'):\n",
    "    # Define inputs\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in3')\n",
    "\n",
    "    # Dense layers\n",
    "    dense1 = tf.keras.layers.Dense(ff_dim, name=f'{prefix}_ff1', activation='relu')\n",
    "    dense2 = tf.keras.layers.Dense(model_dim, name=f'{prefix}_ff2')\n",
    "    drop = tf.keras.layers.Dropout(dropout, name=f'{prefix}_drop')\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
    "\n",
    "    # Apply feed-forward transformation\n",
    "    ffout = drop(dense2(dense1(inputs)))\n",
    "\n",
    "    # Layer normalization and residual connection\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm3')\n",
    "    output = norm(add([inputs, ffout]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f'{prefix}_ff')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "188b4b13-87f0-4b73-b517-15dfd30a0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inputs, targets in train_ds.take(1):\n",
    "#     print(inputs['encode_inp'])\n",
    "#     embed_en = PositionalEmbedding(seq_length, vocab_en , embed_dim = 512)\n",
    "\n",
    "#     en_emb  = embed_en(inputs['encode_inp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea4830-ba8c-4adb-b999-e960f0e32f77",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac670b0b-8232-4490-8916-2330f17625eb",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "446f5439-2576-464f-9a27-0e8774a6cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape, key_dim, ff_dim , dropout = 0.1, prefix='enc', **kwargs):\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        tf.keras.layers.Input(shape = input_shape, dtype = 'float32' , name=f'{prefix}_in0'),\n",
    "        self_attention(input_shape, prefix = prefix, key_dim = key_dim , mask = False, **kwargs),\n",
    "        feed_forward(input_shape , key_dim  , ff_dim , dropout , prefix)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "779f4b26-2ded-496e-a282-274907e75ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = encoder(input_shape = (25,128) , key_dim= 128 ,ff_dim = 512 , num_heads = 8)\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf46c0e-4a15-4877-8d5b-548781068ed6",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d868aca5-6f39-4db9-a7e8-e4acb631d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix='dec', **kwargs):\n",
    "    # Define inputs for decoder\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in0')\n",
    "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_ctx0')\n",
    "    \n",
    "    # Self-attention and cross-attention layers\n",
    "    att_model = self_attention(input_shape, key_dim=key_dim, mask=True, prefix=prefix, **kwargs)\n",
    "    cross_model = cross_attention(input_shape, input_shape, key_dim=key_dim, prefix=prefix, **kwargs)\n",
    "    \n",
    "    # Feed-forward layer\n",
    "    ff_model = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
    "\n",
    "    # Connect layers\n",
    "    x = att_model(inputs)\n",
    "    x = cross_model([context, x])\n",
    "    output = ff_model(x)\n",
    "\n",
    "    # Define decoder model\n",
    "    model = tf.keras.Model(inputs=[inputs, context], outputs=output, name=prefix)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afd10e-1abc-4266-af76-d218b3a2d269",
   "metadata": {},
   "source": [
    "# Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91ec64a8-a51b-4f4e-a3e2-502690f106c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(num_layers, num_heads, seq_length, key_dim, ff_dim, vocab_size_en, vocab_size_fr, dropout=0.1, name='transformer'):\n",
    "    # Define encoder and decoder inputs\n",
    "    input_enc = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='encode_inp')\n",
    "    input_dec = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='decode_inp')\n",
    "\n",
    "    # Positional embeddings for encoder and decoder inputs\n",
    "    emb_enc = PositionalEmbedding(seq_length, vocab_size_en, key_dim, name='embed_enc')\n",
    "    emb_dec = PositionalEmbedding(seq_length, vocab_size_fr, key_dim, name='embed_dec')\n",
    "\n",
    "    # Create encoder and decoder layers\n",
    "    encoders = [encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\", num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "    decoders = [decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\", num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "\n",
    "    # Final dense layer\n",
    "    final = tf.keras.layers.Dense(vocab_size_fr, name='linear')\n",
    "\n",
    "    # Apply encoder and decoder layers to inputs\n",
    "    x1 = emb_enc(input_enc)\n",
    "    x2 = emb_dec(input_dec)\n",
    "    for layer in encoders:\n",
    "        x1 = layer(x1)\n",
    "    for layer in decoders:\n",
    "        x2 = layer([x2, x1])\n",
    "\n",
    "    # Generate output\n",
    "    output = final(x2)\n",
    "\n",
    "    try:\n",
    "        del output.keras_mask\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Define transformer model\n",
    "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a2d68-323c-4aae-9b8c-92a802ac702f",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b62d74-ff81-476a-b5f7-88f5b37b60b3",
   "metadata": {},
   "source": [
    "### Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae48384f-e2c8-47c3-9727-bdffdc1eb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, key_dim, warmup_steps=40000):\n",
    "    super().__init__()\n",
    "    self.key_dim = key_dim\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.d = tf.cast(self.key_dim, tf.float32)\n",
    "\n",
    "  def __call__(self, step):\n",
    "    # Convert step to float32\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    # Calculate learning rate schedule\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Configuration for serialization\n",
    "    config ={\n",
    "      \"key_dim\": self.key_dim,\n",
    "      \"warmup_steps\": self.warmup_steps\n",
    "    }\n",
    "    return config\n",
    "\n",
    "# Define key dimension and create learning rate schedule\n",
    "key_dim = 128\n",
    "lr_schedule = CustomSchedule(key_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e313b95-496c-4bff-a13d-e7bfc2193fb1",
   "metadata": {},
   "source": [
    "### Masked Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b6c9e52-d34e-4b11-86a3-7ee5c011dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  # Create mask for non-padded tokens\n",
    "  mask = label != 0\n",
    "\n",
    "  # Sparse categorical cross-entropy loss\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none'\n",
    "  )\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  # Apply mask to loss\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  # Compute average loss\n",
    "  loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb9c73-74ca-43bb-bf58-8193d87dfa1d",
   "metadata": {},
   "source": [
    "### Masked Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eac548f8-60c7-4560-a8b5-29838fbda1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_accuracy(label, pred):\n",
    "  # Convert predictions to class labels\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "\n",
    "  # Calculate match between labels and predictions\n",
    "  match = label == pred\n",
    "\n",
    "  # Apply mask to match\n",
    "  mask = label != 0\n",
    "  match = match & mask\n",
    "\n",
    "  # Compute accuracy\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a57d20-4fe3-4c17-a6cc-e9f58ecacf29",
   "metadata": {},
   "source": [
    "### Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c29373bc-39b8-457b-ace8-31a9aa65bbff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compile the model with custom loss and metrics\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mmasked_loss, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mmask_accuracy)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model with custom loss and metrics\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=mask_accuracy)\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, epochs=20, validation_data=test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
