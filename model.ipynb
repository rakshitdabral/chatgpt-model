{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4ce8d2f-be57-48ac-bc51-f21cb69a4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcf7c71-9a44-4e5f-8dac-9c35fa2abfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = tf.keras.utils.get_file(\n",
    "    fname='fra-eng.zip',\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22e579a-9778-4e51-9b3f-74bf5f2b68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path(text_file).parent / 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "516c7bd1-3875-4cb0-a11a-d36d1aa1e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(line):\n",
    "    # Normalize unicode characters, strip leading/trailing whitespace, convert to lowercase\n",
    "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
    "    # Handle special characters and add start and end tokens for the target language (French)\n",
    "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\"\\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\"\\1\", line)\n",
    "    eng, fre = line.split(\"\\t\")\n",
    "    fre = '[start] ' + fre + ' [end]'\n",
    "    return eng, fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9254617c-58f3-4ee5-9048-e07cb48fc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as fp:\n",
    "    text_pairs = [normalize(line) for line in fp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b214b8e-1f9f-4ac2-a7a5-aaa81b901f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique tokens for English and French\n",
    "eng_tokens, fre_tokens = set(), set()\n",
    "# Initialize variables to store maximum sequence lengths\n",
    "eng_maxlen, fre_maxlen = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb33934e-fc74-46b1-b2e2-7cdd2148651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, fre in text_pairs:\n",
    "    eng_token, fre_token = eng.split(), fre.split()\n",
    "    eng_maxlen = max(eng_maxlen, len(eng_token))\n",
    "    fre_maxlen = max(fre_maxlen, len(fre_token))\n",
    "    eng_tokens.update(eng_token)\n",
    "    fre_tokens.update(fre_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9b0a45-84db-4f3c-a686-ad30fa5c8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in English: 25380\n",
      "Total tokens in French: 44866\n",
      "Maximum length of English sequence: 47\n",
      "Maximum length of French sequence: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens in English: {len(eng_tokens)}\")\n",
    "print(f\"Total tokens in French: {len(fre_tokens)}\")\n",
    "print(f\"Maximum length of English sequence: {eng_maxlen}\")\n",
    "print(f\"Maximum length of French sequence: {fre_maxlen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978d6d-9a57-4347-8a31-ab30b91de8cd",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27ab2d3-794f-411d-b063-33a9ff454219",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_pairs.pickle\", 'wb') as fp:\n",
    "    pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f11224-f48e-4835-904d-519a018676e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed text pairs\n",
    "with open(\"text_pairs.pickle\", 'rb') as fp:\n",
    "    text_pairs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f946f7a-816e-4d68-b532-7e477a8e0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "random.shuffle(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe1787b1-a9a2-4a16-8d26-82dfdf1b26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "n_val = int(0.15 * len(text_pairs))\n",
    "n_train = len(text_pairs) - 2 * n_val\n",
    "train_pair = text_pairs[:n_train]\n",
    "test_pair = text_pairs[n_train: n_train + n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c993ee-7a63-489c-ad31-c387eb4645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary sizes and sequence length\n",
    "vocab_en = 10000\n",
    "vocab_fr = 20000\n",
    "seq_length = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55861ff-beed-4f6d-bb48-9d5b46270d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextVectorization layers\n",
    "eng_vect = TextVectorization(\n",
    "    max_tokens=vocab_en,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length\n",
    ")\n",
    "\n",
    "fre_vect = TextVectorization(\n",
    "    max_tokens=vocab_fr,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length + 1  # +1 for start token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2021ba26-e63c-4931-9ec2-a5a44d97313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt TextVectorization layers to training data\n",
    "train_eng = [pair[0] for pair in train_pair]\n",
    "train_fre = [pair[1] for pair in train_pair]\n",
    "\n",
    "eng_vect.adapt(train_eng)\n",
    "fre_vect.adapt(train_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68f7a14a-7e1c-4aad-9b2b-e9602fb1fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the vectorization layers and training/test data\n",
    "with open('vectorize.pickle', 'wb') as fp:\n",
    "    data = {'train': train_pair,\n",
    "            'test': test_pair,\n",
    "            'eng_vect': eng_vect.get_config(),\n",
    "            'fre_vect': fre_vect.get_config(),\n",
    "            'eng_weights': eng_vect.get_weights(),\n",
    "            'fre_weights': fre_vect.get_weights()\n",
    "            }\n",
    "    pickle.dump(data, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052d86ff-b167-48c1-b4de-8d6db30240ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load serialized data\n",
    "with open(\"vectorize.pickle\", 'rb') as fp:\n",
    "    data = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e925250e-ffe3-4916-a5a9-e39c5fc1e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve train and test pairs\n",
    "train_pair = data['train']\n",
    "test_pair = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05aa9171-3bc1-47a1-b716-9c78ece29132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct TextVectorization layers\n",
    "eng_vect = TextVectorization.from_config(data['eng_vect'])\n",
    "eng_vect.set_weights(data['eng_weights'])\n",
    "fre_vect = TextVectorization.from_config(data['fre_vect'])\n",
    "fre_vect.set_weights(data['fre_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b73a4ff0-5d53-4462-9b6b-184a47540f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to format dataset\n",
    "def format_dataset(eng, fre):\n",
    "    eng = eng_vect(eng)\n",
    "    fre = fre_vect(fre)\n",
    "    source = {'encode_inp': eng,\n",
    "              'decode_inp': fre[:, :-1]\n",
    "              }\n",
    "    target = fre[:, 1:]\n",
    "    return (source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eb168b6-9a73-449c-a885-74fee8163a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create dataset\n",
    "def make_dataset(pairs, batchsize=64):\n",
    "    eng_text, fre_text = zip(*pairs)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_text), list(fre_text)))\n",
    "    return dataset.shuffle(2048).batch(batchsize).map(format_dataset).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f202c5b-ae91-4dc3-b8e2-37c08c65b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for training and testing\n",
    "train_ds = make_dataset(train_pair)\n",
    "test_ds = make_dataset(test_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78d68-cbef-454c-9606-0678f137d722",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28120abd-3f4d-4b8d-a200-dbfdecbf4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate positional encoding matrix\n",
    "def pos_enc_matrix(L, d, n=10000):\n",
    "    assert d % 2 == 0\n",
    "    d2 = d // 2\n",
    "\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)\n",
    "    i = np.arange(d2).reshape(1, -1)\n",
    "\n",
    "    denom = np.power(n, -i / d2)\n",
    "    args = k * denom\n",
    "\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855d0ffc-5761-4b75-abc4-33d3b5f82ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras layer for positional embedding\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, seq_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        matrix = pos_enc_matrix(seq_length, embed_dim)\n",
    "\n",
    "        self.positional_embedding = tf.constant(matrix, dtype='float32')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + self.positional_embedding\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'seq_length': self.seq_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embed_dim': self.embed_dim\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95d2f661-fa2d-45cf-b008-704c996e1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " ...\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]], shape=(64, 25), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Usage and Validation\n",
    "vocab_en = 10000\n",
    "seq_length = 25\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    embed_en = PositionalEmbedding(seq_length, vocab_en, embed_dim=512)\n",
    "    en_emb = embed_en(inputs['encode_inp'])\n",
    "    print(en_emb._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d33ec-bf26-461e-8d79-5acffd5a0fc6",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd3455a4-f73b-4bdd-a10e-38a013f16e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_shape, prefix='att', mask=False, **kwargs):\n",
    "    # Define inputs\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in1\")\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_att1\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm1')\n",
    "    add = tf.keras.layers.Add(name=f'{prefix}_add1')\n",
    "\n",
    "    # Apply attention mechanism\n",
    "    attout = attention(query=inputs, value=inputs, key=inputs, use_causal_mask=mask)\n",
    "\n",
    "    # Apply normalization and residual connection\n",
    "    output = norm(add([inputs, attout]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f\"{prefix}_att\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781af8f9-3361-41af-aafa-e20e0a701838",
   "metadata": {},
   "source": [
    "### Cross-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d1f1ce-e59d-4452-a008-49f32af6bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(input_shape, context_shape, prefix='att', **kwargs):\n",
    "    # Define inputs\n",
    "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32', name=f\"{prefix}_ctx2\")\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in2')\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f'{prefix}_att2', **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm2')\n",
    "    add = tf.keras.layers.Add(name=f'{prefix}_add2')\n",
    "\n",
    "    # Apply attention mechanism\n",
    "    attout = attention(query=inputs, key=context, value=context)\n",
    "\n",
    "    # Apply normalization and residual connection\n",
    "    output = norm(add([attout, inputs]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[context, inputs], outputs=output, name=f'{prefix}_crs_at')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a3776-b54d-4c2e-81a1-b36dab731ccf",
   "metadata": {},
   "source": [
    "### Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7382b5a8-6a69-4f87-9d5d-0f5cfa75d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(input_shape, model_dim, ff_dim, dropout=.1, prefix='ff'):\n",
    "    # Define inputs\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in3')\n",
    "\n",
    "    # Dense layers\n",
    "    dense1 = tf.keras.layers.Dense(ff_dim, name=f'{prefix}_ff1', activation='relu')\n",
    "    dense2 = tf.keras.layers.Dense(model_dim, name=f'{prefix}_ff2')\n",
    "    drop = tf.keras.layers.Dropout(dropout, name=f'{prefix}_drop')\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
    "\n",
    "    # Apply feed-forward transformation\n",
    "    ffout = drop(dense2(dense1(inputs)))\n",
    "\n",
    "    # Layer normalization and residual connection\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm3')\n",
    "    output = norm(add([inputs, ffout]))\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f'{prefix}_ff')\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
